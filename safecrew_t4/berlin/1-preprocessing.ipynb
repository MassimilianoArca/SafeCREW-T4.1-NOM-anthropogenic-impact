{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berlin Data Preprocessing\n",
    "\n",
    "Time series for the Havel River inflow to the city (Konradshöhe, Messstellennummer 305) and the downstream station (Schleuse Spandau, Messstellennummer 325), DOC and TOC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    median_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(\"..\", \"data\", \"berlin\")\n",
    "raw_data_folder = os.path.join(data_folder, \"raw\")\n",
    "clean_data_folder = os.path.join(data_folder, \"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df = pd.read_csv(\n",
    "    os.path.join(raw_data_folder, \"time-series_surface-water_quality.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df = pd.read_csv(\n",
    "    os.path.join(raw_data_folder, \"time-series_surface-water_flow.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        raw_data_folder,\n",
    "        \"produkt_klima_tag_19480101_20231231_00433.csv\",\n",
    "    ),\n",
    "    sep=\";\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df.rename(\n",
    "    columns={\n",
    "        \"Messstelle\": \"Station\",\n",
    "        \"Messstellennummer\": \"Station ID\",\n",
    "        \"Datum\": \"DateTime\",\n",
    "        \"Einheit\": \"Unit\",\n",
    "        \"Vorzeichen\": \"Sign\",\n",
    "        \"Wert\": \"Value\",\n",
    "        \"Bestimmungsgrenze\": \"LOQ\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "ts_sw_df.drop(\n",
    "    columns=[\n",
    "        \"Entnahmetiefe [m]\",\n",
    "        \"Messmethode\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df.rename(\n",
    "    columns={\n",
    "        \"Messstellennummer\": \"Station ID\",\n",
    "        \"Datum\": \"DateTime\",\n",
    "        \"Einheit\": \"Unit\",\n",
    "        \"Tagesmittelwert\": \"Flow River\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.rename(\n",
    "    columns={\n",
    "        \"STATIONS_ID\": \"Station ID\",\n",
    "        \"MESS_DATUM\": \"DateTime\",\n",
    "        \"  FX\": \"Wind Speed Max (m/s)\",\n",
    "        \"  FM\": \"Wind Speed Mean (m/s)\",\n",
    "        \" RSK\": \"Cumulated Rainfall (mm)\",\n",
    "        \"RSKF\": \"Cumulated Rainfall Type\",\n",
    "        \" SDK\": \"Sunshine Duration (hours)\",\n",
    "        \"SHK_TAG\": \"Snow Height (cm)\",\n",
    "        \"  NM\": \"Cloud Coverage (1/8)\",\n",
    "        \" VPM\": \"Vapor Pressure (hPa)\",\n",
    "        \"  PM\": \"Pressure (hPa)\",\n",
    "        \" TMK\": \"Temperature Mean (°C)\",\n",
    "        \" UPM\": \"Humidity (%)\",\n",
    "        \" TXK\": \"Temperature Max at 2m (°C)\",\n",
    "        \" TNK\": \"Temperature Min at 2m (°C)\",\n",
    "        \" TGK\": \"Temperature Min at 5cm (°C)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset per Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameters that are present for the moment are:\n",
    "variables = {\n",
    "    \"Lufttemperatur\": \"Air Temperature (°C)\",\n",
    "    \"Wassertemperatur\": \"Water Temperature (°C)\",\n",
    "    \"Spektraler Absorptionskoeffizient (SAK) 254nm\": \"UVA254 (1/m)\",\n",
    "    \"Leitfähigkeit\": \"Conductivity (µS/cm)\",\n",
    "    \"Ammonium-Stickstoff\": \"Ammonium (mg/l)\",\n",
    "    \"Sauerstoff-Gehalt\": \"Dissolved Oxygen (mg/l)\",\n",
    "    \"Nitrat-Stickstoff\": \"Nitrate (mg/l)\",\n",
    "    \"pH-Wert\": \"pH\",\n",
    "    \"DOC (Gelöster organischer Kohlenstoff)\": \"DOC (mg/l)\",\n",
    "    \"TOC (Organischer Kohlenstoff)\": \"TOC (mg/l)\",\n",
    "    \"Sulfat\": \"Sulphate (mg/l)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df = ts_sw_df[ts_sw_df[\"Parameter\"].isin(variables.keys())]\n",
    "\n",
    "surface_df[\"Parameter\"] = surface_df[\"Parameter\"].map(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df[\"DateTime\"] = pd.to_datetime(surface_df[\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows where Sign is not nan\n",
    "surface_df[surface_df[\"Sign\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_dict = {}\n",
    "for station in surface_df[\"Station ID\"].unique().tolist():\n",
    "    station_df = surface_df[surface_df[\"Station ID\"] == station]\n",
    "    station_df = station_df.pivot_table(\n",
    "        index=pd.Grouper(\"DateTime\"),\n",
    "        columns=\"Parameter\",\n",
    "        values=\"Value\",\n",
    "    )\n",
    "\n",
    "    stations_dict[station] = station_df\n",
    "\n",
    "stations_dict.pop(105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common columns for all the stations\n",
    "common_columns = set(stations_dict[305].columns)\n",
    "for station_id, station_df in stations_dict.items():\n",
    "    common_columns = common_columns.intersection(station_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_columns:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for station_id, station_df in stations_dict.items():\n",
    "        column_df = station_df[column].copy()\n",
    "\n",
    "        column_df.dropna(inplace=True)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=column_df.index,\n",
    "                y=column_df,\n",
    "                mode=\"lines\",\n",
    "                name=f\"Station {station_id}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=column,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_info_df = pd.DataFrame(\n",
    "    index=pd.Index(\n",
    "        [\n",
    "            \"N Samples\",\n",
    "            \"% Missing Values\",\n",
    "            \"Frequency (days)\",\n",
    "            \"Mean\",\n",
    "            \"Std\",\n",
    "            \"Start Date\",\n",
    "            \"End Date\",\n",
    "        ],\n",
    "        name=\"Info\",\n",
    "    ),\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [surface_df[\"Station ID\"].unique(), variables.values()],\n",
    "        names=[\"Station ID\", \"Parameter\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 305 - Oberhavel-Konradshöhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = stations_dict[305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df[\"DateTime\"] = pd.to_datetime(flow_df[\"DateTime\"])\n",
    "\n",
    "station_flow_df = flow_df[flow_df[\"Station ID\"] == 5815911]\n",
    "\n",
    "station_flow_df = station_flow_df[[\"DateTime\", \"Flow River\"]].set_index(\"DateTime\")\n",
    "\n",
    "station_flow_df.index = station_flow_df.index.date\n",
    "station_df.index = station_df.index.date\n",
    "\n",
    "# merge the flow data with the surface water data for the same date (just date, not time)\n",
    "station_df = station_df.merge(\n",
    "    station_flow_df, left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "station_df.rename(columns={\"Flow River\": \"Flow River Rate (m³/s)\"}, inplace=True)\n",
    "\n",
    "station_df.index = pd.to_datetime(station_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, compute the % of missing values\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    df = station_df[date_range[0] : date_range[1]][column]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"{column}: {missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "for column in station_df.columns:\n",
    "    # compute date range for which the data is available\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    fig = px.line(\n",
    "        station_df,\n",
    "        x=station_df.index,\n",
    "        y=column,\n",
    "        title=f\"{column} at station 305 - Range: {date_range[0].date()} - {date_range[1].date()}\",\n",
    "        labels={\"DateTime\": \"DateTime\", column: column},\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(go.Box(y=column_df[column_df.index.year == year], name=year))\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 305\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.loc[\n",
    "    (station_df[\"DOC (mg/l)\"] <= 0) | (station_df[\"DOC (mg/l)\"] >= 20),\n",
    "    [\"DOC (mg/l)\"],\n",
    "] = np.nan\n",
    "station_df.loc[station_df[\"TOC (mg/l)\"] <= 0, [\"TOC (mg/l)\"]] = np.nan\n",
    "station_df.loc[station_df[\"Flow River Rate (m³/s)\"] < 0, [\"Flow River Rate (m³/s)\"]] = (\n",
    "    np.nan\n",
    ")\n",
    "station_df.loc[station_df[\"Ammonium (mg/l)\"] < 0, [\"Ammonium (mg/l)\"]] = np.nan\n",
    "station_df.loc[station_df[\"Nitrate (mg/l)\"] < 0, [\"Nitrate (mg/l)\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(go.Box(y=column_df[column_df.index.year == year], name=year))\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 305\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the information in the station_info_df\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    start_date = df.dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    end_date = df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df = df[start_date:end_date]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0] * 100\n",
    "\n",
    "    surface_info_df.loc[\"N Samples\", (305, column)] = (\n",
    "        station_df[column].dropna().shape[0]\n",
    "    )\n",
    "    surface_info_df.loc[\"% Missing Values\", (305, column)] = missing_values\n",
    "    surface_info_df.loc[\"Frequency (days)\", (305, column)] = (\n",
    "        station_df.index.to_series().diff().value_counts().index[0].days\n",
    "    )\n",
    "\n",
    "    surface_info_df.loc[\"Mean\", (305, column)] = df.mean()\n",
    "    surface_info_df.loc[\"Std\", (305, column)] = df.std()\n",
    "\n",
    "    surface_info_df.loc[\"Start Date\", (305, column)] = start_date\n",
    "    surface_info_df.loc[\"End Date\", (305, column)] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, compute the % of missing values\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    df = station_df[date_range[0] : date_range[1]][column]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"{column}: {missing_values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"ME\").mean()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"] - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\" if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"]) else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(mean_squared_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"]))\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Percentage Error (MAPE): \" + str(np.round(MAPE, 2)) + \" %\")\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet is used to remove outliers\n",
    "\n",
    "# create copy such that the processed columns do not affect the original dataframe until the end\n",
    "station_df_copy = station_df.copy()\n",
    "\n",
    "station_df_copy = station_df_copy.resample(\"M\").mean()\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").mean()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"] - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\" if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"]) else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove the outliers\n",
    "    forecasting_final = forecasting_final[forecasting_final[\"anomaly\"] == \"No\"]\n",
    "\n",
    "    df = forecasting_final[[\"ds\", \"y\"]]\n",
    "\n",
    "    df.set_index(\"ds\", inplace=True)\n",
    "\n",
    "    df.rename(columns={\"y\": column}, inplace=True)\n",
    "\n",
    "    # redo the resampling since the outliers have been removed and\n",
    "    # some months may have been removed\n",
    "    df = df.resample(\"ME\").mean()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    station_df_copy.loc[df.index, column] = df[column]\n",
    "\n",
    "\n",
    "station_df = station_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the dataset based on the DOC date range\n",
    "start_date = station_df[\"DOC (mg/l)\"].dropna().index.min()\n",
    "end_date = station_df[\"DOC (mg/l)\"].dropna().index.max()\n",
    "\n",
    "station_df = station_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_305_df = station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 325 - Havel-Pichelsdorfer Gemünd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = stations_dict[325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df[\"DateTime\"] = pd.to_datetime(flow_df[\"DateTime\"])\n",
    "\n",
    "station_flow_df = flow_df[flow_df[\"Station ID\"] == 5803200]\n",
    "\n",
    "station_flow_df = station_flow_df[[\"DateTime\", \"Flow River\"]].set_index(\"DateTime\")\n",
    "\n",
    "station_flow_df.index = station_flow_df.index.date\n",
    "station_df.index = station_df.index.date\n",
    "\n",
    "# merge the flow data with the surface water data for the same date (just date, not time)\n",
    "station_df = station_df.merge(\n",
    "    station_flow_df, left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "station_df.rename(columns={\"Flow River\": \"Flow River Rate (m³/s)\"}, inplace=True)\n",
    "\n",
    "station_df.index = pd.to_datetime(station_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "for column in station_df.columns:\n",
    "    fig = px.line(\n",
    "        station_df,\n",
    "        x=station_df.index,\n",
    "        y=column,\n",
    "        title=f\"{column} at station 325\",\n",
    "        labels={\"DateTime\": \"DateTime\", column: column},\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(go.Box(y=column_df[column_df.index.year == year], name=year))\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 325\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.loc[\n",
    "    (station_df[\"DOC (mg/l)\"] > 15) | (station_df[\"DOC (mg/l)\"] < 4.5),\n",
    "    [\"DOC (mg/l)\"],\n",
    "] = np.nan\n",
    "station_df.loc[station_df[\"TOC (mg/l)\"] <= 0, [\"TOC (mg/l)\"]] = np.nan\n",
    "station_df.loc[station_df[\"Flow River Rate (m³/s)\"] < 0, [\"Flow River Rate (m³/s)\"]] = (\n",
    "    np.nan\n",
    ")\n",
    "station_df.loc[station_df[\"Ammonium (mg/l)\"] < 0, [\"Ammonium (mg/l)\"]] = np.nan\n",
    "station_df.loc[station_df[\"Nitrate (mg/l)\"] < 0, [\"Nitrate (mg/l)\"]] = np.nan\n",
    "station_df.loc[station_df[\"pH\"] < 7, [\"pH\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(go.Box(y=column_df[column_df.index.year == year], name=year))\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 105\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the information in the station_info_df\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    start_date = df.dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    end_date = df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df = df[start_date:end_date]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0] * 100\n",
    "\n",
    "    surface_info_df.loc[\"N Samples\", (325, column)] = (\n",
    "        station_df[column].dropna().shape[0]\n",
    "    )\n",
    "    surface_info_df.loc[\"% Missing Values\", (325, column)] = missing_values\n",
    "    surface_info_df.loc[\"Frequency (days)\", (325, column)] = (\n",
    "        station_df.index.to_series().diff().value_counts().index[0].days\n",
    "    )\n",
    "\n",
    "    surface_info_df.loc[\"Mean\", (325, column)] = df.mean()\n",
    "    surface_info_df.loc[\"Std\", (325, column)] = df.std()\n",
    "\n",
    "    surface_info_df.loc[\"Start Date\", (325, column)] = start_date\n",
    "    surface_info_df.loc[\"End Date\", (325, column)] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, compute the % of missing values\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    df = station_df[date_range[0] : date_range[1]][column]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"{column}: {missing_values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through Prophet\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"ME\").mean()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"] - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\" if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"]) else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(mean_squared_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"]))\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Percentage Error (MAPE): \" + str(np.round(MAPE, 2)) + \" %\")\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet is used to remove outliers\n",
    "\n",
    "# create copy such that the processed columns do not affect the original dataframe until the end\n",
    "station_df_copy = station_df.copy()\n",
    "\n",
    "station_df_copy = station_df_copy.resample(\"M\").mean()\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").mean()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"] - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\" if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"]) else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove the outliers\n",
    "    forecasting_final = forecasting_final[forecasting_final[\"anomaly\"] == \"No\"]\n",
    "\n",
    "    df = forecasting_final[[\"ds\", \"y\"]]\n",
    "\n",
    "    df.set_index(\"ds\", inplace=True)\n",
    "\n",
    "    df.rename(columns={\"y\": column}, inplace=True)\n",
    "\n",
    "    # redo the resampling since the outliers have been removed and\n",
    "    # some months may have been removed\n",
    "    df = df.resample(\"M\").mean()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    station_df_copy.loc[df.index, column] = df[column]\n",
    "\n",
    "\n",
    "station_df = station_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the dataset based on the DOC date range\n",
    "start_date = station_df[\"DOC (mg/l)\"].dropna().index.min()\n",
    "end_date = station_df[\"DOC (mg/l)\"].dropna().index.max()\n",
    "\n",
    "station_df = station_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_325_df = station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Unique Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build unique dataframe for all stations\n",
    "# set the number of the station as further variable\n",
    "sw_305_df[\"Station\"] = 305\n",
    "sw_325_df[\"Station\"] = 325\n",
    "\n",
    "sw_305_df.index.name = \"DateTime\"\n",
    "sw_325_df.index.name = \"DateTime\"\n",
    "\n",
    "sw_305_df.reset_index(inplace=True)\n",
    "sw_325_df.reset_index(inplace=True)\n",
    "\n",
    "# merge the dataframes\n",
    "sw_df = pd.concat([sw_305_df, sw_325_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the TOC and BOD columns\n",
    "sw_df.drop(columns=[\"TOC (mg/l)\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meteorological"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_columns = [\"QN_3\", \"QN_4\", \"eor\", \"Cumulated Rainfall Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df[\"DateTime\"] = pd.to_datetime(meteo_df[\"DateTime\"], format=\"%Y%m%d\")\n",
    "\n",
    "meteo_df.set_index(\"DateTime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.loc[meteo_df[\"Cumulated Rainfall (mm)\"] < 0, [\"Cumulated Rainfall (mm)\"]] = (\n",
    "    np.nan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to nan every value that is equal to -999 or -999.0 in the dataframe\n",
    "meteo_df.replace(-999, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_info_df = pd.DataFrame(\n",
    "    index=pd.Index(\n",
    "        [\n",
    "            \"N Samples\",\n",
    "            \"% Missing Values\",\n",
    "            \"Frequency (days)\",\n",
    "            \"Mean\",\n",
    "            \"Std\",\n",
    "            \"Start Date\",\n",
    "            \"End Date\",\n",
    "        ],\n",
    "        name=\"Info\",\n",
    "    ),\n",
    "    columns=pd.Index([\"Parameter\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in meteo_df.columns.difference(diff_columns):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=meteo_df.index,\n",
    "            y=meteo_df[column],\n",
    "            mode=\"lines\",\n",
    "            name=column,\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        title=column,\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in meteo_df.columns.difference(diff_columns):\n",
    "    fig = go.Figure()\n",
    "    column_df = meteo_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(go.Box(y=column_df[column_df.index.year == year], name=year))\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at airport\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in meteo_df.columns.difference(diff_columns):\n",
    "    start_date = meteo_df[column].dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    end_date = meteo_df[column].dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df = meteo_df[start_date:end_date][column]\n",
    "\n",
    "    print(f\"Start date for {column}: {start_date}\")\n",
    "    print(f\"End date for {column}: {end_date}\")\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"Missing values for {column}: {missing_values}\")\n",
    "\n",
    "    frequency = df.index.to_series().diff().value_counts().index[0].days\n",
    "    print(f\"Frequency for {column}: {frequency}\")\n",
    "\n",
    "    meteo_info_df.loc[\"N Samples\", column] = meteo_df[column].dropna().shape[0]\n",
    "    meteo_info_df.loc[\"% Missing Values\", column] = missing_values\n",
    "    meteo_info_df.loc[\"Frequency (days)\", column] = frequency\n",
    "\n",
    "    meteo_info_df.loc[\"Mean\", column] = df.mean()\n",
    "    meteo_info_df.loc[\"Std\", column] = df.std()\n",
    "\n",
    "    meteo_info_df.loc[\"Start Date\", column] = start_date\n",
    "    meteo_info_df.loc[\"End Date\", column] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition\n",
    "\n",
    "for column in meteo_df.columns.difference(diff_columns):\n",
    "    df = meteo_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"ME\").mean()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = meteo_df[column].dropna().index\n",
    "    date_range = meteo_df.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"] - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\" if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"]) else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"])\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(mean_squared_error(forecasting_final[\"yhat\"], forecasting_final[\"y\"]))\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Percentage Error (MAPE): \" + str(np.round(MAPE, 2)) + \" %\")\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to remove outliers\n",
    "meteo_df.drop(columns=diff_columns, inplace=True)\n",
    "\n",
    "meteo_df = meteo_df.resample(\"ME\").mean()\n",
    "\n",
    "meteo_df.interpolate(method=\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to compare the air temperature between the airport and the stations first\n",
    "\n",
    "# plot the air temperature for the airport and the stations\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=meteo_df.index,\n",
    "        y=meteo_df[\"Temperature Mean (°C)\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Airport\",\n",
    "        line=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df[\"DateTime\"],\n",
    "            y=station_df[\"Air Temperature (°C)\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"Station {station_id}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Temperature (°C)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    "    title=\"Temperature\",\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to column\n",
    "meteo_df[\"DateTime\"] = pd.to_datetime(meteo_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df[\"DateTime\"] = meteo_df[\"DateTime\"].astype(int)\n",
    "scaler = MinMaxScaler()\n",
    "meteo_df[\"DateTime\"] = scaler.fit_transform(meteo_df[[\"DateTime\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\n",
    "    f'Q(\"Temperature Mean (°C)\") ~ DateTime',\n",
    "    data=meteo_df[meteo_df.index >= \"1976-08-01\"],\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=meteo_df[meteo_df.index >= \"1976-08-01\"].index,\n",
    "        y=meteo_df[meteo_df.index >= \"1976-08-01\"][\"Temperature Mean (°C)\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Air Temperature (°C)\",\n",
    "        line=dict(color=\"black\"),\n",
    "        opacity=0.5,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=meteo_df[meteo_df.index >= \"1976-08-01\"].index,\n",
    "        y=model.predict(meteo_df[meteo_df.index >= \"1976-08-01\"][\"DateTime\"]),\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"rgb(200, 2, 110)\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=column,\n",
    "    # template='plotly_white',\n",
    "    showlegend=False,\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.6),\n",
    "    margin=dict(l=10, r=10, t=10, b=10),\n",
    ")\n",
    "\n",
    "start_year = meteo_df[meteo_df.index >= \"1976-08-01\"].index.year.min()\n",
    "end_year = meteo_df[meteo_df.index >= \"1976-08-01\"].index.year.max()\n",
    "tickvals = [\n",
    "    pd.Timestamp(f\"{year}-02-26\") for year in range(start_year, end_year + 1, 2)\n",
    "]\n",
    "ticktext = [str(year) for year in range(start_year, end_year + 1, 2)]\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickvals=tickvals,\n",
    "    ticktext=ticktext,\n",
    "    title_text=\"Time\",\n",
    "    tickangle=90,  # Add vertical rotation to x-axis labels\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    range=[-10, 30],\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Temperature (°C)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\n",
    "    f'Q(\"Cumulated Rainfall (mm)\") ~ DateTime',\n",
    "    data=meteo_df[meteo_df.index >= \"1976-08-01\"],\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same with Cumulated Rainfall\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=meteo_df[meteo_df.index >= \"1976-08-01\"].index,\n",
    "        y=meteo_df[meteo_df.index >= \"1976-08-01\"][\"Cumulated Rainfall (mm)\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Cumulated Rainfall (mm)\",\n",
    "        line=dict(color=\"black\"),\n",
    "        opacity=0.5,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=meteo_df[meteo_df.index >= \"1976-08-01\"].index,\n",
    "        y=model.predict(meteo_df[meteo_df.index >= \"1976-08-01\"][\"DateTime\"]),\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"rgb(200, 2, 110)\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=column,\n",
    "    # template='plotly_white',\n",
    "    showlegend=False,\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.6),\n",
    "    margin=dict(l=10, r=10, t=10, b=10),\n",
    ")\n",
    "\n",
    "start_year = meteo_df[meteo_df.index >= \"1976-08-01\"].index.year.min()\n",
    "end_year = meteo_df[meteo_df.index >= \"1976-08-01\"].index.year.max()\n",
    "tickvals = [\n",
    "    pd.Timestamp(f\"{year}-02-26\") for year in range(start_year, end_year + 1, 2)\n",
    "]\n",
    "ticktext = [str(year) for year in range(start_year, end_year + 1, 2)]\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickvals=tickvals,\n",
    "    ticktext=ticktext,\n",
    "    title_text=\"Time\",\n",
    "    tickangle=90,  # Add vertical rotation to x-axis labels\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    range=[0, 7],\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Cumulated Rainfall (mm)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pearson correlation\n",
    "\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    start_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].min()\n",
    "    end_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].max()\n",
    "\n",
    "    # take the common date range with the airport\n",
    "    start_date = max(start_date, meteo_df.index.min())\n",
    "    end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "    airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "    # take the common date range with the station\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "    station_df = station_df[\n",
    "        (station_df[\"DateTime\"] >= start_date) & (station_df[\"DateTime\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # compute pearson correlation\n",
    "    corr, _ = stats.pearsonr(\n",
    "        airport_df[\"Temperature Mean (°C)\"],\n",
    "        station_df[\"Air Temperature (°C)\"],\n",
    "    )\n",
    "\n",
    "    rmse = np.sqrt(\n",
    "        mean_squared_error(\n",
    "            airport_df[\"Temperature Mean (°C)\"], station_df[\"Air Temperature (°C)\"]\n",
    "        )\n",
    "    )\n",
    "    rmse = rmse / (\n",
    "        airport_df[\"Temperature Mean (°C)\"].max()\n",
    "        - airport_df[\"Temperature Mean (°C)\"].min()\n",
    "    )\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=airport_df.index,\n",
    "            y=airport_df[\"Temperature Mean (°C)\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Airport\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"],\n",
    "            y=sw_df[sw_df[\"Station\"] == station_id][\"Air Temperature (°C)\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"Station {station_id}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add the correlation to the plot\n",
    "    fig.add_annotation(\n",
    "        x=0.01,\n",
    "        y=0.95,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        text=f\"Pearson Correlation: {corr:.2f}\",\n",
    "        showarrow=False,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_annotation(\n",
    "        x=0.01,\n",
    "        y=0.90,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        text=f\"RMSE: {rmse:.2f}\",\n",
    "        showarrow=False,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Air Temperature (°C)\",\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of the air temperature between the airport and the stations\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    start_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].min()\n",
    "    end_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].max()\n",
    "\n",
    "    # take the common date range with the airport\n",
    "    start_date = max(start_date, meteo_df.index.min())\n",
    "    end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "    airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "    # take the common date range with the station\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "    station_df = station_df[\n",
    "        (station_df[\"DateTime\"] >= start_date) & (station_df[\"DateTime\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=airport_df[\"Temperature Mean (°C)\"],\n",
    "            y=station_df[\"Air Temperature (°C)\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Data\",\n",
    "            marker=dict(size=8, color=\"blue\", opacity=0.7),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add line on bisector\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[-10, 40],\n",
    "            y=[-10, 40],\n",
    "            mode=\"lines\",\n",
    "            name=\"Bisector\",\n",
    "            line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Airport\",\n",
    "        yaxis_title=f\"Station {station_id}\",\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        title=\"Air Temperature\",\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the sw_df first to have unique indices\n",
    "sw_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation is high between the airport and the stations,\n",
    "# so we can add the airport data variables to the stations\n",
    "\n",
    "# add the rainfall data to the stations\n",
    "sw_df[\"Cumulated Rainfall (mm)\"] = np.nan\n",
    "\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    start_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].min()\n",
    "    end_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].max()\n",
    "\n",
    "    # take the common date range with the airport\n",
    "    start_date = max(start_date, meteo_df.index.min())\n",
    "    end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "    airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "    # take the common date range with the station\n",
    "    # Identify the indices in sw_df that match the station_id and are within the date range\n",
    "    indices = sw_df[\n",
    "        (sw_df[\"Station\"] == station_id)\n",
    "        & (sw_df[\"DateTime\"] >= start_date)\n",
    "        & (sw_df[\"DateTime\"] <= end_date)\n",
    "    ].index\n",
    "\n",
    "    # Directly update sw_df for the matching indices\n",
    "    sw_df.loc[indices, \"Cumulated Rainfall (mm)\"] = airport_df[\n",
    "        \"Cumulated Rainfall (mm)\"\n",
    "    ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df[\"Cumulated Rainfall (mm)\"].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same with the temperature, we are going to use the mean temperature of the airport for the stations\n",
    "for stations_id in sw_df[\"Station\"].unique():\n",
    "    start_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].min()\n",
    "    end_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].max()\n",
    "\n",
    "    # take the common date range with the airport\n",
    "    start_date = max(start_date, meteo_df.index.min())\n",
    "    end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "    airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "    # take the common date range with the station\n",
    "    # Identify the indices in sw_df that match the station_id and are within the date range\n",
    "    indices = sw_df[\n",
    "        (sw_df[\"Station\"] == station_id)\n",
    "        & (sw_df[\"DateTime\"] >= start_date)\n",
    "        & (sw_df[\"DateTime\"] <= end_date)\n",
    "    ].index\n",
    "\n",
    "    # Directly update sw_df for the matching indices\n",
    "    sw_df.loc[indices, \"Air Temperature (°C)\"] = airport_df[\n",
    "        \"Temperature Mean (°C)\"\n",
    "    ].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df.drop(columns=[\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the date range for every station and variable\n",
    "\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "\n",
    "    for column in station_df.columns.difference([\"DateTime\", \"Station\"]):\n",
    "        start_date = station_df[[\"DateTime\", column]].dropna()[\"DateTime\"].min()\n",
    "        end_date = station_df[[\"DateTime\", column]].dropna()[\"DateTime\"].max()\n",
    "\n",
    "        print(f\"Station {station_id} - {column}\")\n",
    "        print(f\"Start date: {start_date}\")\n",
    "        print(f\"End date: {end_date}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ammonium last value\n",
    "sw_df[\"Ammonium (mg/l)\"].ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# make it start and finish with the same month\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "\n",
    "    for column in station_df.columns.difference(\n",
    "        [\"DateTime\", \"Station\"]\n",
    "    ):\n",
    "        df = station_df[[\"DateTime\", column]].copy()\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        start_index = df[df['DateTime'].dt.month == df['DateTime'].max().month].index[0]\n",
    "        \n",
    "        # set to nan until the start index\n",
    "        df.loc[:start_index, column] = np.nan\n",
    "        \n",
    "        station_df.loc[df.index, column] = df[column]\n",
    "        \n",
    "        # update the sw_df\n",
    "        sw_df.loc[station_df.index, column] = station_df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "sw_df.dropna(\n",
    "    subset=station_df.columns.difference([\"DateTime\", \"Station\"]), how=\"all\", inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df.to_excel(os.path.join(clean_data_folder, \"berlin.xlsx\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-t4-1-nom-anthropogenic-impact-1EhQKKig-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
